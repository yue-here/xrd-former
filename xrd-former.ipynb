{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XRD transformer\n",
    "Standard seq2seq model architecture based on [language translation model](https://pytorch.org/tutorials/beginner/translation_transformer.html).\n",
    "\n",
    "### Model setup\n",
    "Concept: lattice parameters arise solely from peak positions, so learn relationship between peak positions (e.g. (100) and (200) peaks) via self attention. Positional encoding is intrinsic to peak positions already, so does not need to be learned. Output via seq2seq: lattice parameters arise from whole set of peaks.\n",
    "\n",
    "Use [regression transformer](https://www.nature.com/articles/s42256-023-00639-z) to frame problem as a seq2seq problem. Numbers are tokenized based on digits and positions, and an embedding preserving distances between numbers is applied. (This works surprisingly well in the paper). Numbers can then be generated as a sequence of tokens before being decoded back to numbers.\n",
    "\n",
    "Example: 12.34 --> ['\\_1\\_1\\_', '\\_2\\_0\\_', '\\._', '\\_3\\_-1\\_', '\\_4\\_-2\\_']\n",
    "\n",
    "\n",
    "Source\n",
    "* Start with [q, I] data\n",
    "* Peak positions (q) are binned to integers - initial tests used bin sizes of 0.0005 in q\n",
    "* Convert bin positions to the regression transformer numerical encoding\n",
    "* Different input lengths are dynamically padded during batching\n",
    "\n",
    "Targets\n",
    "* Start with [a, b, c] lattice data\n",
    "* Tokenize data using the regression transformer numerical encoding\n",
    "* Different output lengths are dynamically padded during batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Iterable, List\n",
    "from timeit import default_timer as timer\n",
    "import tqdm\n",
    "import numerical_encodings # adapted from regression transformer paper\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import torchtext\n",
    "\n",
    "# Check if GPU is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from files\n",
    "\n",
    "Load data from pickles\n",
    "\n",
    "* Lattice parameters are sorted smallest to largest.\n",
    "* Peaks are unsorted(?)\n",
    "* Max q is 1.0 (0.99999 or something - maybe it's normalized to 1.0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "162\n",
      "143804\n"
     ]
    }
   ],
   "source": [
    "def load_data(file1_path, file2_path):\n",
    "    with open(file1_path, 'rb') as file1:\n",
    "        list1 = pickle.load(file1)\n",
    "    with open(file2_path, 'rb') as file2:\n",
    "        list2 = pickle.load(file2)\n",
    "    return list1, list2\n",
    "\n",
    "xdata_filename = \"./data/qI.pickle\"\n",
    "ydata_filename = \"./data/lps.pickle\"\n",
    "X, y = load_data(xdata_filename, ydata_filename)\n",
    "\n",
    "# Some entries are 0 length so limit X to entries with more than 10 elements\n",
    "X = [x for x in X if len(x) > 10]\n",
    "\n",
    "lens = [len(thing) for thing in X]\n",
    "\n",
    "print(min(lens))\n",
    "print(max(lens))\n",
    "print(len(lens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detokenization routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_by_values(my_dict, values_list):\n",
    "    keys_list = [key for value in values_list for key, val in my_dict.items() if val == value]\n",
    "    return keys_list\n",
    "\n",
    "def split_list(input_list, delimiters):\n",
    "    '''\n",
    "    Split a list into sublists based on a list of delimiters\n",
    "    E.g. for the transfomer output which is separated by |\n",
    "    '''\n",
    "    result = []\n",
    "    temp_list = []\n",
    "\n",
    "    for item in input_list:\n",
    "        if item not in delimiters:\n",
    "            temp_list.append(item)\n",
    "        else:\n",
    "            if temp_list:  # Check if temp_list is not empty\n",
    "                result.append(temp_list)\n",
    "                temp_list = []\n",
    "\n",
    "    # Add the last temp_list if not empty and not added yet\n",
    "    if temp_list and temp_list not in result:\n",
    "        result.append(temp_list)\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_floats(seq):\n",
    "    out = []\n",
    "    delimiters = ['|', '<eos>', '<bos>', '<a>', '<b>', '<c>']\n",
    "    seq_split = split_list(seq, delimiters)\n",
    "    for i in range(len(seq_split)):\n",
    "        float_string = \"\".join([token.split(\"_\")[1] for token in seq_split[i]])\n",
    "        out.append(float(float_string))\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split targets into numerical encoding\n",
    "\n",
    "PropertyTokenizer from regression transformer paper.\n",
    "\n",
    "Pass to model: vocab + list of token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually modified\n",
    "class PropertyTokenizer:\n",
    "    \"\"\"Run a property tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Constructs a PropertyTokenizer.\"\"\"\n",
    "        self.regex = re.compile(r\"\\s*(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenization of a property.\n",
    "        Args:\n",
    "            text: text to tokenize.\n",
    "        Returns:\n",
    "            extracted tokens.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        matched = self.regex.match(text)\n",
    "        if matched:\n",
    "            sign, units, dot, decimals = matched.groups()\n",
    "            if sign:\n",
    "                tokens += [f\"_{sign}_\"]\n",
    "            tokens += [\n",
    "                f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n",
    "            ][::-1]\n",
    "            if dot:\n",
    "                tokens += [f\"_{dot}_\"]\n",
    "            if decimals:\n",
    "                tokens += [\n",
    "                    f\"_{number}_-{position}_\"\n",
    "                    for position, number in enumerate(decimals, 1)\n",
    "                ]\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_floats(self, floats: List[float]) -> List[List[str]]:\n",
    "        \"\"\"Tokenization of a list of floats.\n",
    "        Args:\n",
    "            floats: list of floats to tokenize.\n",
    "        Returns:\n",
    "            List of tokenized floats.\n",
    "        \"\"\"\n",
    "        tokens_list = []\n",
    "        for num in floats:\n",
    "            tokens = self.tokenize(str(num))\n",
    "            tokens_list.append(\"|\")\n",
    "            for i in tokens:\n",
    "                tokens_list.append(i)\n",
    "        return tokens_list\n",
    "    \n",
    "    from typing import List\n",
    "\n",
    "    def tokenize_floats_tgt(self, floats: List[float]) -> List[str]:\n",
    "        \"\"\"Tokenization of a list of floats.\n",
    "        Args:\n",
    "            floats: list of floats to tokenize.\n",
    "        Returns:\n",
    "            List of tokenized floats.\n",
    "        \"\"\"\n",
    "        if len(floats) != 3:\n",
    "            raise ValueError(\"The input list must have exactly 3 elements\")\n",
    "\n",
    "        tokens_list = []\n",
    "        prefix_list = ['<a>', '<b>', '<c>']\n",
    "        \n",
    "        for idx, num in enumerate(floats):\n",
    "            tokens = self.tokenize(str(num))\n",
    "            tokens_list.append(prefix_list[idx])\n",
    "            for i in tokens:\n",
    "                tokens_list.append(i)\n",
    "        \n",
    "        return tokens_list\n",
    "\n",
    "max_q = 1\n",
    "\n",
    "def bin_q(arr, max_q, n=0.0005):\n",
    "    '''\n",
    "    Bins the q values in an array of shape (n, 2) into bins of size `n`\n",
    "    '''\n",
    "    bin_edges = np.arange(0.0, max_q + n, n)\n",
    "\n",
    "    # Use `np.digitize` to bin the q values into integers\n",
    "    binned_q = np.digitize(arr[:,0], bin_edges) - 1 # subtract 1 to make the index 0-based\n",
    "    \n",
    "    return binned_q\n",
    "\n",
    "class RaggedTensorDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def generate_vocab(min_range, max_range, decimal_places=0, special_symbols=['<pad>', '<bos>', '<eos>', '|', '<a>', '<b>', '<c>']):\n",
    "    token_set = set()\n",
    "    \n",
    "    max_digits_left = len(str(max_range).split(\".\")[0])\n",
    "    max_digits_right = decimal_places\n",
    "\n",
    "    for position in range(max_digits_left, -max_digits_right - 1, -1):\n",
    "        for digit in range(10):\n",
    "            token = f\"_{digit}_{position}_\"\n",
    "            token_set.add(token)\n",
    "    \n",
    "    if decimal_places > 0:\n",
    "        token_set.add(\"_._\")\n",
    "\n",
    "    tokens = special_symbols + sorted(list(token_set))\n",
    "\n",
    "    # create a dictionary that maps each token to an index\n",
    "    token_to_index = {}\n",
    "    for i in range(len(tokens)):\n",
    "        token_to_index[tokens[i]] = i\n",
    "\n",
    "    return token_to_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full vocab generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# find maximum number of decimal places in y\n",
    "max_decimals = 0\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(y[i])):\n",
    "        if len(str(y[i][j]).split('.')[1]) > max_decimals:\n",
    "            max_decimals = len(str(y[i][j]).split('.')[1])\n",
    "print(max_decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<bos>': 1, '<eos>': 2, '|': 3, '<a>': 4, '<b>': 5, '<c>': 6, '_0_0_': 7, '_0_1_': 8, '_0_2_': 9, '_0_3_': 10, '_0_4_': 11, '_1_0_': 12, '_1_1_': 13, '_1_2_': 14, '_1_3_': 15, '_1_4_': 16, '_2_0_': 17, '_2_1_': 18, '_2_2_': 19, '_2_3_': 20, '_2_4_': 21, '_3_0_': 22, '_3_1_': 23, '_3_2_': 24, '_3_3_': 25, '_3_4_': 26, '_4_0_': 27, '_4_1_': 28, '_4_2_': 29, '_4_3_': 30, '_4_4_': 31, '_5_0_': 32, '_5_1_': 33, '_5_2_': 34, '_5_3_': 35, '_5_4_': 36, '_6_0_': 37, '_6_1_': 38, '_6_2_': 39, '_6_3_': 40, '_6_4_': 41, '_7_0_': 42, '_7_1_': 43, '_7_2_': 44, '_7_3_': 45, '_7_4_': 46, '_8_0_': 47, '_8_1_': 48, '_8_2_': 49, '_8_3_': 50, '_8_4_': 51, '_9_0_': 52, '_9_1_': 53, '_9_2_': 54, '_9_3_': 55, '_9_4_': 56}\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# Generate vocab_X\n",
    "min_range = 0\n",
    "max_range = 9999\n",
    "vocab_X = generate_vocab(min_range, max_range, 0)\n",
    "print(vocab_X)\n",
    "print(len(vocab_X))\n",
    "\n",
    "# Generate vocab_y\n",
    "min_range = 0\n",
    "max_range = 999\n",
    "vocab_y = generate_vocab(min_range, max_range, max_decimals)\n",
    "# print(vocab_y)\n",
    "# print(len(vocab_y))\n",
    "\n",
    "PAD_IDX, BOS_IDX, EOS_IDX, PIPE_IDX = 0, 1, 2, 3\n",
    "A_IDX, B_IDX, C_IDX = 4, 5, 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate token IDs for each dataset\n",
    "* The floatencoder converts tensors of IDs to embedding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PropertyTokenizer()\n",
    "\n",
    "tokenized_y = [tokenizer.tokenize_floats_tgt(y[i]) for i in range(len(y))]\n",
    "token_ids_y = [[vocab_y[token] for token in tokenized_y[i]] for i in range(len(tokenized_y))]\n",
    "\n",
    "binned_X = [bin_q(X[i], max_q) for i in range(len(X))]\n",
    "tokenized_X = [tokenizer.tokenize_floats(binned_X[i]) for i in range(len(binned_X))]\n",
    "token_ids_X = [[vocab_X[token] for token in tokenized_X[i]] for i in range(len(tokenized_X))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ragged tensor dataset\n",
    "* Padding and special token appending happens in the collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 129423\n",
      "Validation size: 7190\n",
      "Test size: 7191\n"
     ]
    }
   ],
   "source": [
    "ragged_dataset = RaggedTensorDataset(token_ids_X, token_ids_y)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the proportions for train, validation, and test splits\n",
    "train_ratio = 0.90\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.05\n",
    "\n",
    "# Calculate the sizes for each split\n",
    "train_size = int(train_ratio * len(ragged_dataset))\n",
    "val_size = int(val_ratio * len(ragged_dataset))\n",
    "test_size = len(ragged_dataset) - train_size - val_size\n",
    "\n",
    "# Perform the train-validation-test split\n",
    "train_dataset, val_dataset, test_dataset = random_split(ragged_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# print sizes\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model\n",
    "Using this as basis https://pytorch.org/tutorials/beginner/translation_transformer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 src_vocab: dict,\n",
    "                 tgt_vocab: dict,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size) # outputs probability distribution over target vocabulary\n",
    "\n",
    "        # Note - bug when \"numerical_encodings.\" was omitted, causing model to try to call forward()\n",
    "        self.src_numerical_encoding = numerical_encodings.FloatEncoding(\n",
    "            num_embeddings = src_vocab_size,\n",
    "            embedding_dim = emb_size,\n",
    "            vocab = src_vocab)\n",
    "        self.tgt_numerical_encoding = numerical_encodings.FloatEncoding(\n",
    "            num_embeddings = tgt_vocab_size,\n",
    "            embedding_dim = emb_size,\n",
    "            vocab = tgt_vocab)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                tgt: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.src_numerical_encoding(src) # src_tok_emb acts on the input tensor. Instead of positional, use numerical\n",
    "        tgt_emb = self.tgt_numerical_encoding(tgt) # tgt_tok_emb acts on the output tensor\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask) # this is calling self.forward\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.src_numerical_encoding(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.tgt_numerical_encoding(tgt), memory, tgt_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define padding and causal masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define collation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function takes in a list of samples and returns separate source and target batches.\n",
    "    \"\"\"\n",
    "    # Get the inputs and labels separately\n",
    "    # Inputs are lists and converted to tensors here\n",
    "    src_batch = [torch.tensor(sample[0]) for sample in batch]\n",
    "    tgt_batch = [torch.tensor(sample[1]) for sample in batch]\n",
    "    \n",
    "    # add BOS_IDX to end and EOS_IDX to start of tgt_batch\n",
    "    tgt_batch = [torch.cat((torch.tensor([BOS_IDX]), tgt_batch[i], torch.tensor([EOS_IDX]))) for i in range(len(tgt_batch))]\n",
    "\n",
    "    # # Reserve the right to pad later\n",
    "    src_batch = pad_sequence(src_batch, batch_first=False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=False)\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\y_w_u\\Google Drive\\SLAC\\code experiments\\xrd-former\\repo\\numerical_encodings.py:172: UserWarning: The inferred maximum float (90000) is used for normalizing all float embeddings which might result in diminishing embeddings.\n",
      "  warnings.warn(\n",
      "c:\\Users\\y_w_u\\Google Drive\\SLAC\\code experiments\\xrd-former\\repo\\numerical_encodings.py:172: UserWarning: The inferred maximum float (9000) is used for normalizing all float embeddings which might result in diminishing embeddings.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_X)\n",
    "TGT_VOCAB_SIZE = len(vocab_y)\n",
    "SRC_VOCAB = vocab_X\n",
    "TGT_VOCAB = vocab_y\n",
    "EMB_SIZE = 16\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 64\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, SRC_VOCAB, TGT_VOCAB, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.253, Val loss: 2.106, Epoch time = 175.985s\n",
      "Epoch: 2, Train loss: 1.830, Val loss: 1.629, Epoch time = 167.281s\n",
      "Epoch: 3, Train loss: 1.628, Val loss: 1.570, Epoch time = 167.637s\n",
      "Epoch: 4, Train loss: 1.586, Val loss: 1.551, Epoch time = 167.469s\n",
      "Epoch: 5, Train loss: 1.569, Val loss: 1.542, Epoch time = 167.768s\n",
      "Epoch: 6, Train loss: 1.559, Val loss: 1.536, Epoch time = 170.318s\n",
      "Epoch: 7, Train loss: 1.551, Val loss: 1.532, Epoch time = 167.076s\n",
      "Epoch: 8, Train loss: 1.546, Val loss: 1.528, Epoch time = 167.536s\n",
      "Epoch: 9, Train loss: 1.542, Val loss: 1.525, Epoch time = 167.810s\n",
      "Epoch: 10, Train loss: 1.539, Val loss: 1.523, Epoch time = 168.182s\n",
      "Epoch: 11, Train loss: 1.536, Val loss: 1.521, Epoch time = 168.280s\n",
      "Epoch: 12, Train loss: 1.533, Val loss: 1.520, Epoch time = 167.012s\n",
      "Epoch: 13, Train loss: 1.531, Val loss: 1.518, Epoch time = 166.768s\n",
      "Epoch: 14, Train loss: 1.529, Val loss: 1.517, Epoch time = 166.881s\n",
      "Epoch: 15, Train loss: 1.528, Val loss: 1.516, Epoch time = 166.727s\n",
      "Epoch: 16, Train loss: 1.526, Val loss: 1.515, Epoch time = 191.409s\n",
      "Epoch: 17, Train loss: 1.525, Val loss: 1.514, Epoch time = 188.328s\n",
      "Epoch: 18, Train loss: 1.523, Val loss: 1.513, Epoch time = 188.581s\n",
      "Epoch: 19, Train loss: 1.522, Val loss: 1.512, Epoch time = 197.079s\n",
      "Epoch: 20, Train loss: 1.521, Val loss: 1.512, Epoch time = 192.678s\n",
      "Epoch: 21, Train loss: 1.520, Val loss: 1.511, Epoch time = 187.864s\n",
      "Epoch: 22, Train loss: 1.520, Val loss: 1.511, Epoch time = 187.889s\n",
      "Epoch: 23, Train loss: 1.519, Val loss: 1.510, Epoch time = 188.058s\n",
      "Epoch: 24, Train loss: 1.518, Val loss: 1.510, Epoch time = 187.359s\n",
      "Epoch: 25, Train loss: 1.517, Val loss: 1.509, Epoch time = 196.866s\n",
      "Epoch: 26, Train loss: 1.517, Val loss: 1.509, Epoch time = 192.281s\n",
      "Epoch: 27, Train loss: 1.516, Val loss: 1.509, Epoch time = 191.932s\n",
      "Epoch: 28, Train loss: 1.516, Val loss: 1.508, Epoch time = 191.700s\n",
      "Epoch: 29, Train loss: 1.515, Val loss: 1.508, Epoch time = 191.802s\n",
      "Epoch: 30, Train loss: 1.515, Val loss: 1.508, Epoch time = 191.830s\n",
      "Epoch: 31, Train loss: 1.514, Val loss: 1.507, Epoch time = 192.777s\n",
      "Epoch: 32, Train loss: 1.514, Val loss: 1.507, Epoch time = 191.989s\n",
      "Epoch: 33, Train loss: 1.514, Val loss: 1.507, Epoch time = 191.856s\n",
      "Epoch: 34, Train loss: 1.513, Val loss: 1.506, Epoch time = 193.614s\n",
      "Epoch: 35, Train loss: 1.513, Val loss: 1.506, Epoch time = 193.346s\n",
      "Epoch: 36, Train loss: 1.512, Val loss: 1.506, Epoch time = 192.125s\n",
      "Epoch: 37, Train loss: 1.512, Val loss: 1.506, Epoch time = 197.257s\n",
      "Epoch: 38, Train loss: 1.512, Val loss: 1.505, Epoch time = 219.920s\n",
      "Epoch: 39, Train loss: 1.511, Val loss: 1.505, Epoch time = 215.189s\n",
      "Epoch: 40, Train loss: 1.511, Val loss: 1.505, Epoch time = 215.029s\n",
      "Epoch: 41, Train loss: 1.511, Val loss: 1.505, Epoch time = 210.815s\n",
      "Epoch: 42, Train loss: 1.510, Val loss: 1.504, Epoch time = 195.297s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     start_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m----> 6\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(transformer, optimizer)\n\u001b[0;32m      7\u001b[0m     end_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m      8\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(transformer)\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(logits\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logits\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), tgt_out\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     22\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 24\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     25\u001b[0m     losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m losses \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(train_dataloader))\n",
      "File \u001b[1;32mc:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\y_w_u\\anaconda3\\envs\\xrd-former\\Lib\\site-packages\\torch\\optim\\adam.py:505\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(max_exp_avg_sq_sqrt, eps)\n\u001b[0;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    506\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    507\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    # Save model every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(transformer.state_dict(), f\"transformer_{epoch}.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model\n",
    "\n",
    "Load saved model and test on examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a torch model from a .pth file\n",
    "model_weights_path = \"transformer_40.pt\" \n",
    "transformer.load_state_dict(torch.load(model_weights_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def greedy_decode_verbose(model, src, src_mask, max_len, start_symbol,top_k=3):\n",
    "    def get_keys_by_values(vocab, token):\n",
    "        return [k for k, v in vocab.items() if v == token]\n",
    "    \n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "\n",
    "        # Get the top k tokens and their probabilities\n",
    "        top_probs, top_tokens = torch.topk(prob, top_k, dim=1)\n",
    "        top_probs = torch.softmax(top_probs, dim=-1)\n",
    "        \n",
    "        # Print the top k tokens and their probabilities\n",
    "        print(f\"Step {i}: Top {top_k} tokens and probabilities:\")\n",
    "        for j in range(top_k):\n",
    "            token = top_tokens[0, j].item()\n",
    "            token_str = get_keys_by_values(vocab_y, token)[0]\n",
    "            print(f\"Token '{token_str}': Probability {top_probs[0, j].item()}\")\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src, verbose=False):\n",
    "    model.eval()\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    if verbose:\n",
    "        tgt_tokens = greedy_decode_verbose(\n",
    "            model,  src, src_mask, max_len=30, start_symbol=BOS_IDX).flatten()\n",
    "    else:\n",
    "        tgt_tokens = greedy_decode(\n",
    "            model,  src, src_mask, max_len=30, start_symbol=BOS_IDX).flatten()\n",
    "    return tgt_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which tokens the model is predicting\n",
    "* the structure of the numbers is being captured (P near 1 for tags and \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Top 3 tokens and probabilities:\n",
      "Token '<a>': Probability 0.9999988079071045\n",
      "Token '_5_0_': Probability 6.563274723703216e-07\n",
      "Token '_4_0_': Probability 4.707397351921827e-07\n",
      "Step 1: Top 3 tokens and probabilities:\n",
      "Token '_1_1_': Probability 0.6457002758979797\n",
      "Token '_9_0_': Probability 0.1870238035917282\n",
      "Token '_8_0_': Probability 0.16727592051029205\n",
      "Step 2: Top 3 tokens and probabilities:\n",
      "Token '_0_0_': Probability 0.4196949005126953\n",
      "Token '_1_0_': Probability 0.3270508050918579\n",
      "Token '_2_0_': Probability 0.25325435400009155\n",
      "Step 3: Top 3 tokens and probabilities:\n",
      "Token '_._': Probability 1.0\n",
      "Token '<eos>': Probability 6.363818361165841e-10\n",
      "Token '_1_1_': Probability 2.546460819985441e-10\n",
      "Step 4: Top 3 tokens and probabilities:\n",
      "Token '_0_-1_': Probability 0.3384811580181122\n",
      "Token '_1_-1_': Probability 0.3347078561782837\n",
      "Token '_2_-1_': Probability 0.32681092619895935\n",
      "Step 5: Top 3 tokens and probabilities:\n",
      "Token '_3_-2_': Probability 0.33856016397476196\n",
      "Token '_8_-2_': Probability 0.3318674564361572\n",
      "Token '_1_-2_': Probability 0.32957231998443604\n",
      "Step 6: Top 3 tokens and probabilities:\n",
      "Token '_4_-3_': Probability 0.3391610383987427\n",
      "Token '_2_-3_': Probability 0.33268606662750244\n",
      "Token '_8_-3_': Probability 0.32815292477607727\n",
      "Step 7: Top 3 tokens and probabilities:\n",
      "Token '<b>': Probability 0.8098183870315552\n",
      "Token '_1_-4_': Probability 0.09713052958250046\n",
      "Token '_7_-4_': Probability 0.09305112063884735\n",
      "Step 8: Top 3 tokens and probabilities:\n",
      "Token '_1_1_': Probability 0.9017492532730103\n",
      "Token '_2_1_': Probability 0.09310443699359894\n",
      "Token '_3_1_': Probability 0.005146319512277842\n",
      "Step 9: Top 3 tokens and probabilities:\n",
      "Token '_2_0_': Probability 0.3526104688644409\n",
      "Token '_1_0_': Probability 0.3385421931743622\n",
      "Token '_3_0_': Probability 0.3088473677635193\n",
      "Step 10: Top 3 tokens and probabilities:\n",
      "Token '_._': Probability 1.0\n",
      "Token '<eos>': Probability 1.0680737183577094e-08\n",
      "Token '_8_-2_': Probability 2.2626120976809716e-09\n",
      "Step 11: Top 3 tokens and probabilities:\n",
      "Token '_6_-1_': Probability 0.33662667870521545\n",
      "Token '_4_-1_': Probability 0.33398088812828064\n",
      "Token '_7_-1_': Probability 0.3293924033641815\n",
      "Step 12: Top 3 tokens and probabilities:\n",
      "Token '_3_-2_': Probability 0.334682434797287\n",
      "Token '_2_-2_': Probability 0.333003968000412\n",
      "Token '_8_-2_': Probability 0.3323136270046234\n",
      "Step 13: Top 3 tokens and probabilities:\n",
      "Token '_4_-3_': Probability 0.34073933959007263\n",
      "Token '_8_-3_': Probability 0.3299121558666229\n",
      "Token '_5_-3_': Probability 0.32934850454330444\n",
      "Step 14: Top 3 tokens and probabilities:\n",
      "Token '<c>': Probability 0.9779992699623108\n",
      "Token '_1_-4_': Probability 0.011287952773272991\n",
      "Token '_9_-4_': Probability 0.010712821036577225\n",
      "Step 15: Top 3 tokens and probabilities:\n",
      "Token '_1_1_': Probability 0.5521427392959595\n",
      "Token '_2_1_': Probability 0.3670533299446106\n",
      "Token '_3_1_': Probability 0.08080396801233292\n",
      "Step 16: Top 3 tokens and probabilities:\n",
      "Token '_4_0_': Probability 0.36754029989242554\n",
      "Token '_3_0_': Probability 0.32317954301834106\n",
      "Token '_5_0_': Probability 0.309280127286911\n",
      "Step 17: Top 3 tokens and probabilities:\n",
      "Token '_._': Probability 1.0\n",
      "Token '<eos>': Probability 1.4481109644748358e-08\n",
      "Token '_5_-4_': Probability 7.632373622001865e-10\n",
      "Step 18: Top 3 tokens and probabilities:\n",
      "Token '_6_-1_': Probability 0.335476815700531\n",
      "Token '_4_-1_': Probability 0.3331068456172943\n",
      "Token '_7_-1_': Probability 0.33141636848449707\n",
      "Step 19: Top 3 tokens and probabilities:\n",
      "Token '_3_-2_': Probability 0.33382314443588257\n",
      "Token '_1_-2_': Probability 0.33342069387435913\n",
      "Token '_2_-2_': Probability 0.3327561318874359\n",
      "Step 20: Top 3 tokens and probabilities:\n",
      "Token '_4_-3_': Probability 0.33876535296440125\n",
      "Token '_8_-3_': Probability 0.33175936341285706\n",
      "Token '_9_-3_': Probability 0.3294752836227417\n",
      "Step 21: Top 3 tokens and probabilities:\n",
      "Token '<eos>': Probability 0.9966533780097961\n",
      "Token '_1_-4_': Probability 0.0017344700172543526\n",
      "Token '_9_-4_': Probability 0.0016121434746310115\n"
     ]
    }
   ],
   "source": [
    "src = test_dataset[0][0]\n",
    "src = torch.tensor(src).unsqueeze(1)\n",
    "out = translate(transformer, src, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(n, verbose=False):\n",
    "    src = test_dataset[n][0]\n",
    "    src = torch.tensor(src).unsqueeze(1)\n",
    "    src = translate(transformer, src, verbose=False)\n",
    "    src = get_keys_by_values(vocab_y, src.tolist())\n",
    "    src = extract_floats(src)\n",
    "    tgt = get_keys_by_values(vocab_y, test_dataset[n][1])\n",
    "    tgt = extract_floats(tgt)\n",
    "    # convert to numpy arrays\n",
    "    src = np.array(src)\n",
    "    tgt = np.array(tgt)\n",
    "    # calculate the absolute error\n",
    "    error = np.abs(src - tgt)\n",
    "    # calculate the mean absolute error\n",
    "    mae = np.mean(error)\n",
    "    # calculate the mean squared error\n",
    "    mse = np.mean(error**2)\n",
    "    # calculate the root mean squared error\n",
    "    rmse = np.sqrt(mse)\n",
    "    # calculate the mean absolute percentage error\n",
    "    mape = np.mean(np.abs(error / tgt)) * 100\n",
    "    # calculate the coefficient of determination\n",
    "    ss_res = np.sum((tgt - src)**2)\n",
    "    ss_tot = np.sum((tgt - np.mean(tgt))**2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    if verbose:\n",
    "        # summarize\n",
    "        print('MAE: %.3f' % mae)\n",
    "        print('MSE: %.3f' % mse)\n",
    "        print('RMSE: %.3f' % rmse)\n",
    "        print('MAPE: %.3f' % mape)\n",
    "        print('R2: %.3f' % r2)\n",
    "\n",
    "    return src, tgt, mape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted and actual lattice parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length 202, predicted:[10.034 12.634 14.634], actual:[ 6.5144 10.05   17.824 ], mape:32.54555317667957\n",
      "dataset length 318, predicted:[10.034 12.634 14.634], actual:[ 5.6059  9.3515 19.8996], mape:46.85071557120319\n",
      "dataset length 165, predicted:[10.034 12.634 14.634], actual:[ 8.202 14.803 19.783], mape:21.005282727796395\n",
      "dataset length 167, predicted:[10.034 12.634 14.634], actual:[14.75  15.3   15.543], mape:18.415336597597452\n",
      "dataset length 153, predicted:[10.034 12.634 14.634], actual:[ 7.458 22.887 24.652], mape:39.99204159316957\n",
      "dataset length 91, predicted:[10.034 12.634 14.634], actual:[ 7.146 19.443 19.762], mape:33.7944413824394\n",
      "dataset length 124, predicted:[10.034 12.434 14.434], actual:[ 8.1959 10.6113 12.9909], mape:16.904194707792872\n",
      "dataset length 174, predicted:[10.034 12.634 14.634], actual:[ 9.2492 10.2439 13.9779], mape:12.170276808011689\n",
      "dataset length 184, predicted:[10.034 12.634 14.634], actual:[ 8.229 14.784 28.713], mape:28.503636397869194\n",
      "dataset length 316, predicted:[10.034 12.634 14.634], actual:[12.6402 21.9538 25.3094], mape:35.08326876989487\n",
      "dataset length 159, predicted:[10.034 12.434 14.434], actual:[ 9.7835 10.3369 41.011 ], mape:29.217503892125517\n",
      "dataset length 163, predicted:[10.034 12.634 14.634], actual:[ 5.695   7.5345 28.7029], mape:64.29574696038435\n",
      "dataset length 249, predicted:[10.034 12.634 14.634], actual:[13.2926 16.75   16.8467], mape:20.74061627892719\n",
      "dataset length 187, predicted:[10.034 12.634 14.634], actual:[ 7.6563 10.9386 12.4438], mape:21.385148257472\n",
      "dataset length 66, predicted:[10.034 12.43  14.43 ], actual:[ 8.185  12.0619 13.2149], mape:11.61226192501833\n",
      "dataset length 122, predicted:[10.034 12.434 14.434], actual:[ 7.0613 20.904  37.524 ], mape:48.05033103167708\n",
      "dataset length 152, predicted:[10.034 12.634 14.634], actual:[12.0385 15.1889 16.6681], mape:15.22504409600767\n",
      "dataset length 121, predicted:[10.034 12.434 14.434], actual:[10.623  20.393  23.8095], mape:27.983270116657998\n",
      "dataset length 62, predicted:[10.034 12.43  14.43 ], actual:[10.228 10.419 13.688], mape:8.872945689514754\n",
      "dataset length 178, predicted:[10.034 12.634 14.634], actual:[12.334 13.972 15.097], mape:10.4302568621785\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    a, b, mape = compare_results(i)\n",
    "    print(f'dataset length {len(test_dataset[i][0])}, predicted:{a}, actual:{b}, mape:{mape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
